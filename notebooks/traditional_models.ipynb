{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import numpyro\n",
    "from numpyro.infer import SVI, Trace_ELBO, MCMC, NUTS, Predictive\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer.reparam import NeuTraReparam, LocScaleReparam\n",
    "from numpyro.infer.autoguide import AutoDiagonalNormal\n",
    "from numpyro.diagnostics import split_gelman_rubin\n",
    "from numpyro.handlers import reparam\n",
    "from numpyro.optim import Adam\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, device_get\n",
    "\n",
    "numpyro.util.set_host_device_count(5)\n",
    "device_num = jax.device_count('cpu')\n",
    "numpyro.enable_x64()\n",
    "prng_seed = random.PRNGKey(33425453)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary least squares for simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gravity\n",
    "$$\n",
    "\\hat M_{i,j,t} = \\kappa \\frac{ P_{i,t} P_{j,t} } { D_{i,j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_count = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1894.3986174749848 +/- 7.201791653105702\n",
      "R-squared: 0.4019724092629852 +/- 0.0009483982499523122\n",
      "CPC: 0.6337717765697434 +/- 0.0012180169622381947\n",
      "CPCD: 0.7563012823319919 +/- 0.0009056481365264334\n"
     ]
    }
   ],
   "source": [
    "cpc_gravity , cpcd_gravity, mae_gravity , r_squared_gravity = np.empty(path_count), np.empty(path_count), np.empty(path_count), np.empty(path_count)\n",
    "for path in df.path_ind.unique()[:path_count]:\n",
    "\n",
    "    df_train_filtered = df_train.query(\"path_ind == @path & M_ij != 0\")\n",
    "    X_train = df_train_filtered[['P_i','P_j','D_ij']].astype('float')\n",
    "    y_train = df_train_filtered.M_ij\n",
    "    gravity_ols = sm.OLS(np.log(y_train), sm.add_constant(np.log(X_train))).fit()\n",
    "\n",
    "    df_test_filtered = df_test.query(\"path_ind == @path & M_ij != 0\")\n",
    "    X_test = df_test_filtered[['P_i','P_j','D_ij']].astype('float')\n",
    "    y_test = df_test_filtered.M_ij\n",
    "\n",
    "    pred = np.exp( gravity_ols.predict( sm.add_constant(np.log(X_test)) ) )\n",
    "\n",
    "    cpc_gravity[path] = cpc(y_test, pred) \n",
    "    cpcd_gravity[path] = cpcd(y_test, pred, df_test_filtered.D_ij) \n",
    "    mae_gravity[path] = mae(y_test, pred) \n",
    "    r_squared_gravity[path] = r_squared(y_test, pred) \n",
    "\n",
    "print(\"MAE:\", mae_gravity.mean()            , \"+/-\", norm.ppf(.975) * mae_gravity.std(ddof=1) / np.sqrt(path_count)      )\n",
    "print(\"R-squared:\", r_squared_gravity.mean(), \"+/-\", norm.ppf(.975) * r_squared_gravity.std(ddof=1) / np.sqrt(path_count)      )\n",
    "print(\"CPC:\", cpc_gravity.mean()             , \"+/-\", norm.ppf(.975) * cpc_gravity.std(ddof=1) / np.sqrt(path_count)      )\n",
    "print(\"CPCD:\", cpcd_gravity.mean()           , \"+/-\", norm.ppf(.975) * cpcd_gravity.std(ddof=1) / np.sqrt(path_count)      )\n",
    "# print(\"BIC:\", gravity_ols.bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>M_ij</td>       <th>  R-squared:         </th> <td>   0.509</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.509</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   9809.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 08 Nov 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:07:22</td>     <th>  Log-Likelihood:    </th> <td> -46128.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 28432</td>      <th>  AIC:               </th> <td>9.226e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 28428</td>      <th>  BIC:               </th> <td>9.230e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  -12.3496</td> <td>    0.176</td> <td>  -70.269</td> <td> 0.000</td> <td>  -12.694</td> <td>  -12.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P_i</th>   <td>    0.7673</td> <td>    0.007</td> <td>  107.588</td> <td> 0.000</td> <td>    0.753</td> <td>    0.781</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P_j</th>   <td>    0.7883</td> <td>    0.007</td> <td>  111.096</td> <td> 0.000</td> <td>    0.774</td> <td>    0.802</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>D_ij</th>  <td>   -0.6102</td> <td>    0.009</td> <td>  -66.760</td> <td> 0.000</td> <td>   -0.628</td> <td>   -0.592</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>2827.209</td> <th>  Durbin-Watson:     </th> <td>   1.697</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5291.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-0.674</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 4.627</td>  <th>  Cond. No.          </th> <td>    549.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   M_ij   R-squared:                       0.509\n",
       "Model:                            OLS   Adj. R-squared:                  0.509\n",
       "Method:                 Least Squares   F-statistic:                     9809.\n",
       "Date:                Wed, 08 Nov 2023   Prob (F-statistic):               0.00\n",
       "Time:                        17:07:22   Log-Likelihood:                -46128.\n",
       "No. Observations:               28432   AIC:                         9.226e+04\n",
       "Df Residuals:                   28428   BIC:                         9.230e+04\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -12.3496      0.176    -70.269      0.000     -12.694     -12.005\n",
       "P_i            0.7673      0.007    107.588      0.000       0.753       0.781\n",
       "P_j            0.7883      0.007    111.096      0.000       0.774       0.802\n",
       "D_ij          -0.6102      0.009    -66.760      0.000      -0.628      -0.592\n",
       "==============================================================================\n",
       "Omnibus:                     2827.209   Durbin-Watson:                   1.697\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5291.449\n",
       "Skew:                          -0.674   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.627   Cond. No.                         549.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gravity_ols.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radiation\n",
    "$$\n",
    "\\hat M_{i,j,t} = \\hat M_{i,t} \\frac { P_{i,t} P_{j,t}  } {  (P_{i,t} + S_{i,j,t}) (P_{i,t} + P_{j,t} + S_{i,j,t})  }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1657.7314500935186 +/- 5.830838789400446\n",
      "R-squared: 0.5274364369669536 +/- 0.002694356279335885\n",
      "CPC: 0.6951472820854869 +/- 0.0010415672821489576\n",
      "CPCD: 0.816930285281288 +/- 0.0016372229054317463\n"
     ]
    }
   ],
   "source": [
    "cpc_r , cpcd_r, mae_r , r_squared_r = np.empty(path_count), np.empty(path_count), np.empty(path_count), np.empty(path_count)\n",
    "for path in df.path_ind.unique()[:path_count]:\n",
    "\n",
    "    df_train_filtered = df_train.query(\"path_ind == @path & M_ij != 0\")\n",
    "    X_train = df_train_filtered[['P_i','P_j','SP_ij']].astype('float')\n",
    "    X_train['P_i + SP_ij'] = X_train.P_i + X_train.SP_ij\n",
    "    X_train['P_i + P_j + SP_ij'] = X_train.P_i + X_train.P_j + X_train.SP_ij\n",
    "    X_train.drop('SP_ij',axis=1,inplace=True)\n",
    "    y_train = df_train_filtered.M_ij\n",
    "\n",
    "    rad_ols = sm.OLS(np.log(y_train), sm.add_constant(np.log(X_train))).fit()\n",
    "\n",
    "    df_test_filtered = df_test.query(\"path_ind == @path & M_ij != 0\")\n",
    "    X_test = df_test_filtered[['P_i','P_j','SP_ij']].astype('float')\n",
    "    X_test['P_i + SP_ij'] = X_test.P_i + X_test.SP_ij\n",
    "    X_test['P_i + P_j + SP_ij'] = X_test.P_i + X_test.P_j + X_test.SP_ij\n",
    "    X_test.drop('SP_ij',axis=1,inplace=True)    \n",
    "    y_test = df_test_filtered.M_ij\n",
    "\n",
    "    pred = np.exp( rad_ols.predict( sm.add_constant(np.log(X_test)) ))\n",
    "\n",
    "    cpc_r[path] =  cpc(y_test, pred)\n",
    "    cpcd_r[path] = cpcd(y_test, pred, df_test_filtered.D_ij)\n",
    "    mae_r[path] = mae(y_test, pred)\n",
    "    r_squared_r[path] = r_squared(y_test, pred)\n",
    "\n",
    "print(\"MAE:\", mae_r.mean()            , \"+/-\", norm.ppf(.975) * mae_r.std(ddof=1) / np.sqrt(path_count)      )\n",
    "print(\"R-squared:\", r_squared_r.mean(), \"+/-\", norm.ppf(.975) * r_squared_r.std(ddof=1) / np.sqrt(path_count)      )\n",
    "print(\"CPC:\", cpc_r.mean()             , \"+/-\", norm.ppf(.975) * cpc_r.std(ddof=1) / np.sqrt(path_count)      )\n",
    "print(\"CPCD:\", cpcd_r.mean()           , \"+/-\", norm.ppf(.975) * cpcd_r.std(ddof=1) / np.sqrt(path_count)      )\n",
    "# print(\"BIC:\", rad_ols.bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>M_ij</td>       <th>  R-squared:         </th> <td>   0.592</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.592</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.029e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 08 Nov 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:08:32</td>     <th>  Log-Likelihood:    </th> <td> -43499.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 28432</td>      <th>  AIC:               </th> <td>8.701e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 28427</td>      <th>  BIC:               </th> <td>8.705e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>   -5.5948</td> <td>    0.200</td> <td>  -27.980</td> <td> 0.000</td> <td>   -5.987</td> <td>   -5.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P_i</th>               <td>    0.8808</td> <td>    0.007</td> <td>  134.670</td> <td> 0.000</td> <td>    0.868</td> <td>    0.894</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P_j</th>               <td>    0.8029</td> <td>    0.007</td> <td>  115.460</td> <td> 0.000</td> <td>    0.789</td> <td>    0.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P_i + SP_ij</th>       <td>   -0.5171</td> <td>    0.039</td> <td>  -13.101</td> <td> 0.000</td> <td>   -0.594</td> <td>   -0.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P_i + P_j + SP_ij</th> <td>   -0.1918</td> <td>    0.045</td> <td>   -4.244</td> <td> 0.000</td> <td>   -0.280</td> <td>   -0.103</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>3926.421</td> <th>  Durbin-Watson:     </th> <td>   1.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>8836.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-0.821</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.183</td>  <th>  Cond. No.          </th> <td>1.03e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.03e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   M_ij   R-squared:                       0.592\n",
       "Model:                            OLS   Adj. R-squared:                  0.592\n",
       "Method:                 Least Squares   F-statistic:                 1.029e+04\n",
       "Date:                Wed, 08 Nov 2023   Prob (F-statistic):               0.00\n",
       "Time:                        17:08:32   Log-Likelihood:                -43499.\n",
       "No. Observations:               28432   AIC:                         8.701e+04\n",
       "Df Residuals:                   28427   BIC:                         8.705e+04\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "const                -5.5948      0.200    -27.980      0.000      -5.987      -5.203\n",
       "P_i                   0.8808      0.007    134.670      0.000       0.868       0.894\n",
       "P_j                   0.8029      0.007    115.460      0.000       0.789       0.817\n",
       "P_i + SP_ij          -0.5171      0.039    -13.101      0.000      -0.594      -0.440\n",
       "P_i + P_j + SP_ij    -0.1918      0.045     -4.244      0.000      -0.280      -0.103\n",
       "==============================================================================\n",
       "Omnibus:                     3926.421   Durbin-Watson:                   1.800\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             8836.344\n",
       "Skew:                          -0.821   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.183   Cond. No.                     1.03e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.03e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rad_ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lagrange multiplier statistic', 591.2844606665697),\n",
       " ('p-value', 1.192459743295018e-126),\n",
       " ('f-value', 150.7091169095696),\n",
       " ('f p-value', 6.931790228605662e-128)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.compat import lzip\n",
    "\n",
    "# Conduct the Breusch-Pagan test\n",
    "names = ['Lagrange multiplier statistic', 'p-value',\n",
    "         'f-value', 'f p-value']\n",
    "\n",
    "test_result = sm.stats.het_breuschpagan(rad_ols.resid, rad_ols.model.exog)\n",
    " \n",
    "lzip(names, test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference for Generalized Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ggm(p_i, d_ij, y, scaled_factors_origin, scaled_factors_destination):\n",
    "\n",
    "    λ = numpyro.sample(\"λ\", dist.HalfNormal(1))\n",
    "    w = numpyro.sample(\"w\", dist.Laplace(0, λ * jnp.ones(scaled_factors_origin.shape[1])))\n",
    "    \n",
    "    U_i = jnp.dot(w , scaled_factors_origin.T)\n",
    "    U_j = jnp.dot(w , scaled_factors_destination.T)\n",
    "\n",
    "    Β = jnp.array([1,1,1,1,-1]) * numpyro.sample(\"Β\", dist.HalfNormal(1*jnp.ones(5))) \n",
    "    σ = numpyro.sample(\"σ\", dist.HalfNormal(1))\n",
    "\n",
    "    M_ij = Β[0] + jnp.dot(Β[1:], jnp.array([jnp.log(1+p_i), jnp.log(1+U_i), jnp.log(1+U_j), jnp.log(1 + d_ij )]))\n",
    "\n",
    "    numpyro.sample(\"flow\", dist.Normal(M_ij, σ), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ggm(p_i, d_ij, y, scaled_factors_origin, scaled_factors_destination):\n",
    "\n",
    "    λ = numpyro.sample(\"λ\", dist.HalfNormal(1))\n",
    "    w = numpyro.sample(\"w\", dist.Laplace(0, λ * jnp.ones(scaled_factors_origin.shape[1])))\n",
    "    \n",
    "    U_i = jnp.dot(w , scaled_factors_origin.T)\n",
    "    U_j = jnp.dot(w , scaled_factors_destination.T)\n",
    "\n",
    "    # factors = jnp.array([jnp.log(1+p_i), jnp.log(1+U_i), jnp.log(1+U_j), jnp.log(1 + d_ij )])\n",
    "\n",
    "    # Β_param = np.linalg.inv(factors.T @ factors) @ factors.T @ y\n",
    "    # α_param = y.mean() - Β_param @ factors.mean(axis=0)\n",
    "\n",
    "    Β = numpyro.sample(\"Β\", dist.Normal(np.zeros(5),1)) \n",
    "    σ = numpyro.sample(\"σ\", dist.HalfNormal(1))\n",
    "\n",
    "    M_ij = Β[0] + jnp.dot(Β[1:], jnp.array([jnp.log(1+p_i), jnp.log(1+U_i), jnp.log(1+U_j), jnp.log(1 + d_ij )]))\n",
    "\n",
    "    numpyro.sample(\"flow\", dist.Normal(M_ij, σ), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:15<00:00, 1662.28it/s, init loss: 726828.0506, avg. loss [23751-25000]: 70780.9158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain 1 -65294.192347590935\n",
      "Chain 2 -65177.47619206775\n",
      "Chain 3 -65204.446788175155\n",
      "Chain 4 -65136.35520489906\n",
      "Chain 5 -65442.55790829731\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Number of divergences: 24",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/ajcutuli/Desktop/MURI_Migration/MURI-migration/aric_notebooks/traditional_models.ipynb Cell 15\u001b[0m line \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ajcutuli/Desktop/MURI_Migration/MURI-migration/aric_notebooks/traditional_models.ipynb#X36sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mChain 5\u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39mpoten[\u001b[39m4\u001b[39m,:]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ajcutuli/Desktop/MURI_Migration/MURI-migration/aric_notebooks/traditional_models.ipynb#X36sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m extra_fields \u001b[39m=\u001b[39m mcmc\u001b[39m.\u001b[39mget_extra_fields()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ajcutuli/Desktop/MURI_Migration/MURI-migration/aric_notebooks/traditional_models.ipynb#X36sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdiverging\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m extra_fields, \u001b[39m\"\u001b[39m\u001b[39mNumber of divergences: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(jnp\u001b[39m.\u001b[39msum(extra_fields[\u001b[39m\"\u001b[39m\u001b[39mdiverging\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ajcutuli/Desktop/MURI_Migration/MURI-migration/aric_notebooks/traditional_models.ipynb#X36sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msampling posteriors for model of path \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(path \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, path_count))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ajcutuli/Desktop/MURI_Migration/MURI-migration/aric_notebooks/traditional_models.ipynb#X36sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m mcmc\u001b[39m.\u001b[39mrun(prng_seed, X_train\u001b[39m.\u001b[39mP_i\u001b[39m.\u001b[39mvalues, X_train\u001b[39m.\u001b[39mD_ij\u001b[39m.\u001b[39mvalues, y_train, scaled_factors_origin, scaled_factors_destination)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Number of divergences: 24"
     ]
    }
   ],
   "source": [
    "cpc_gg , cpcd_gg, mae_gg , r_squared_gg = np.empty(path_count), np.empty(path_count), np.empty(path_count), np.empty(path_count)\n",
    "# weights_gg, coeff_gg = np.empty((path_count, len(features))), np.empty((path_count, 4))\n",
    "posteriors_gg = []\n",
    "for path in df.path_ind.unique()[:path_count]:\n",
    "\n",
    "    X_train = df_train.query(\"path_ind == @path\").loc[:,'P_i':].astype('float')\n",
    "    y_train = np.log(1 + df_train.query(\"path_ind == @path\").M_ij.values)\n",
    "\n",
    "    tmp = np.log(1 + X_train)\n",
    "    scaled_factors = 1 / (1 + np.exp(- (tmp - tmp.mean(axis=0)) / tmp.std(ddof=1, axis=0) ))\n",
    "\n",
    "    features = {'population': ['P_i','P_j'],\n",
    "            'housing': ['H_i','H_j'],\n",
    "            'income': ['I_i','I_j'],\n",
    "            'affordability': ['AF_i','AF_j'],\n",
    "            'area': ['A_i','A_j'],\n",
    "            'density': ['rho_i','rho_j'],\n",
    "            'climate risk': ['ADC_i','ADC_j']\n",
    "            }\n",
    "\n",
    "    features_origin = list(np.array(list(features.values()))[:,0])\n",
    "    features_destination = list(np.array(list(features.values()))[:,1])\n",
    "\n",
    "    scaled_factors_origin = scaled_factors[features_origin].values\n",
    "    scaled_factors_destination = scaled_factors[features_destination].values\n",
    "\n",
    "    # config = {\"w\":LocScaleReparam(), \"Β\":LocScaleReparam()}\n",
    "    # reparam_ggm = reparam(ggm, config)   \n",
    "    guide = AutoDiagonalNormal(ggm)\n",
    "    svi = SVI(ggm, guide, Adam(1e-4), Trace_ELBO())\n",
    "    svi_result = svi.run(prng_seed, 25000, X_train.P_i.values, X_train.D_ij.values, y_train, scaled_factors_origin, scaled_factors_destination, progress_bar=True)\n",
    "    neutra = NeuTraReparam(guide, svi_result.params)\n",
    "\n",
    "    reparam_ggm = neutra.reparam(ggm)\n",
    "    # config = {\"w\":LocScaleReparam(), \"Β\":LocScaleReparam()}\n",
    "    # reparam_ggm = reparam(reparam_ggm, config)\n",
    "    nuts_kernel = NUTS(reparam_ggm, init_strategy=numpyro.infer.init_to_sample, target_accept_prob=.99, max_tree_depth=10)\n",
    "    num_warmup = 200\n",
    "    num_samples = 1000\n",
    "    num_chains = 5\n",
    "    thinning = 1\n",
    "\n",
    "    mcmc = MCMC(nuts_kernel, num_warmup=num_warmup, num_samples=num_samples, num_chains=num_chains, thinning=thinning, progress_bar=False)\n",
    "    mcmc.warmup(prng_seed, X_train.P_i.values, X_train.D_ij.values, y_train, scaled_factors_origin, scaled_factors_destination, collect_warmup=True, extra_fields=(\"potential_energy\",))\n",
    "    poten = mcmc.get_extra_fields(group_by_chain=True)[\"potential_energy\"]\n",
    "    print(\"Chain 1\", np.mean(-poten[0,:]))\n",
    "    if num_chains==5:\n",
    "        print(\"Chain 2\", np.mean(-poten[1,:]))\n",
    "        print(\"Chain 3\", np.mean(-poten[2,:]))\n",
    "        print(\"Chain 4\", np.mean(-poten[3,:]))\n",
    "        print(\"Chain 5\", np.mean(-poten[4,:]))\n",
    "    extra_fields = mcmc.get_extra_fields()\n",
    "    assert \"diverging\" not in extra_fields, \"Number of divergences: {}\".format(jnp.sum(extra_fields[\"diverging\"]))\n",
    "\n",
    "    print(\"sampling posteriors for model of path {}/{}\".format(path + 1, path_count))\n",
    "    mcmc.run(prng_seed, X_train.P_i.values, X_train.D_ij.values, y_train, scaled_factors_origin, scaled_factors_destination)\n",
    "\n",
    "    posteriors_gg += [neutra.transform_sample(mcmc.get_samples(group_by_chain=True)['auto_shared_latent'])]\n",
    "\n",
    "    for name, value in posteriors_gg[-1].items():\n",
    "        rhat = split_gelman_rubin(value).max()\n",
    "        print(\"{} parameter has max R hat {}\".format(name, rhat))\n",
    "        # assert rhat < 1.1, \"R hat too large\"\n",
    "\n",
    "    X_test = df_test.query(\"path_ind == @path\").loc[:,'P_i':].astype('float')\n",
    "    y_test = df_test.query(\"path_ind == @path\").M_ij.values\n",
    "\n",
    "    tmp = np.log(1 + X_test)\n",
    "    scaled_factors = 1 / (1 + np.exp(- (tmp - tmp.mean(axis=0)) / tmp.std(ddof=1, axis=0) ))\n",
    "\n",
    "    predictive = Predictive(reparam_ggm, posteriors_gg[-1], return_sites=[\"flow\"])\n",
    "    samples_predictive = predictive(prng_seed, X_test.P_i.values, X_test.D_ij.values, None, scaled_factors[features_origin].values, scaled_factors[features_destination].values)\n",
    "\n",
    "    pred = np.exp( samples_predictive[\"flow\"].mean(axis=0) ) - 1\n",
    "    \n",
    "    mae_gg[path] = mae(y_test, pred )\n",
    "    r_squared_gg[path] = r_squared(y_test, pred)\n",
    "    cpc_gg[path] = cpc(y_test, pred)\n",
    "    cpcd_gg[path] = cpcd(y_test, pred, df_test.query(\"path_ind == @path\").D_ij)\n",
    "    \n",
    "    print(mae_gg[path])\n",
    "\n",
    "print(\"MAE: {}, +/- {}\".format(mae_gg.mean(), norm.ppf(.975) * mae_gg.std(ddof=1) / np.sqrt(path_count) ) )\n",
    "print(\"R-squared: {}, +/- {}\".format(r_squared_gg.mean(), norm.ppf(.975) * r_squared_gg.std(ddof=1) / np.sqrt(path_count) ) )\n",
    "print(\"CPC: {}, +/- {}\".format(cpc_gg.mean(), norm.ppf(.975) * cpc_gg.std(ddof=1) / np.sqrt(path_count) ) )\n",
    "print(\"CPCD: {}, +/- {}\".format(cpcd_gg.mean(), norm.ppf(.975) * cpcd_gg.std(ddof=1) / np.sqrt(path_count) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2645.8250455281195"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_gg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'population': ['P_i', 'P_j'],\n",
       " 'housing': ['H_i', 'H_j'],\n",
       " 'income': ['I_i', 'I_j'],\n",
       " 'affordability': ['AF_i', 'AF_j'],\n",
       " 'area': ['A_i', 'A_j'],\n",
       " 'density': ['rho_i', 'rho_j'],\n",
       " 'climate risk': ['ADC_i', 'ADC_j']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.43474673, 0.68037996, 0.01160111, 0.41280846, 1.47442949,\n",
       "       0.04364658, 0.25848849], dtype=float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posteriors_gg[0]['w'].reshape((-1,7)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.14861701, 0.16268577, 0.18217934, 0.16209478, 0.27205569,\n",
       "       0.12189468, 0.20443296], dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posteriors_gg[0]['w'].reshape((-1,7)).std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/posteriors_gg.pkl', 'wb') as f:\n",
    "    pickle.dump(posteriors_gg, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Radiation\n",
    "$$\n",
    "M_{i,j} = M_{i} \\frac { U_i U_j  } {  (U_i + S^U_{i,j}) (U_{i} + U_{j} + S^U_{i,j})  }\n",
    "$$\n",
    "$$\n",
    "U_i = \\sum_k w_k f_{k:i}\n",
    "$$\n",
    "$$\n",
    "M_i = \\kappa P_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grm(X, y, scaled_factors, local_region_sum, features_origin, features_destination):\n",
    "    \n",
    "    # w_sigma = numpyro.sample(\"w_sigma\", dist.HalfNormal(10*jnp.ones(len(features_origin))))\n",
    "    # w_mu = numpyro.sample(\"w_mu\", dist.Normal(0, 10*jnp.ones(len(features_origin))))\n",
    "    # coeff_sigma = numpyro.sample(\"coeff_sigma\", dist.HalfNormal(10*jnp.ones(6)))\n",
    "    # coeff_mu = numpyro.sample(\"coeff_mu\", dist.Normal(0, 10*jnp.ones(6)))\n",
    "\n",
    "    # w = numpyro.sample(\"w\", dist.Normal(0, 10*jnp.ones(len(features_origin))))\n",
    "    # coeff = numpyro.sample(\"coeff\", dist.Normal(coeff_mu, coeff_sigma))\n",
    "\n",
    "    w = numpyro.sample(\"w\", dist.Normal(0, 10*jnp.ones(len(features_origin))))\n",
    "    coeff = numpyro.sample(\"coeff\", dist.Normal(0, 10*jnp.ones(6)))\n",
    "    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(10))\n",
    "\n",
    "    U_i = jnp.dot(w , scaled_factors[features_origin].values.T)\n",
    "    U_j = jnp.dot(w , scaled_factors[features_destination].values.T)\n",
    "    SU_ij = jnp.dot(w, local_region_sum.T)\n",
    "\n",
    "    M_ij = coeff[0] + jnp.dot(coeff[1:], jnp.array([np.log( 1 + X.P_i.values), jnp.log(1 + U_i) , jnp.log(1 + U_j),jnp.log(1 + U_i + SU_ij) , jnp.log(1 + U_i + U_j + SU_ij)  ]) )\n",
    "\n",
    "    numpyro.sample(\"flow\", dist.Normal(M_ij, sigma), obs=y)\n",
    "\n",
    "    λ = numpyro.sample(\"λ\", dist.HalfNormal(10))\n",
    "    w = numpyro.sample(\"w\", dist.Laplace(0, λ * jnp.ones(scaled_factors_origin.shape[1])))\n",
    "    \n",
    "    U_i = jnp.dot(w , scaled_factors_origin.T)\n",
    "    U_j = jnp.dot(w , scaled_factors_destination.T)\n",
    "\n",
    "    Β = jnp.array([1,1,1,-1,-1]) * numpyro.sample(\"Β\", dist.HalfNormal(10*jnp.ones(5))) \n",
    "    σ = numpyro.sample(\"σ\", dist.HalfNormal(10))\n",
    "\n",
    "    M_ij = Β[0] + jnp.dot(Β[1:], jnp.array([jnp.log(1+U_i), jnp.log(1+U_j), jnp.log(1 + d_ij )]))\n",
    "\n",
    "    numpyro.sample(\"flow\", dist.Normal(M_ij, σ), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running path 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2419.70it/s, init loss: 2762002.2500, avg. loss [9501-10000]: 64334.8555]\n",
      "sample: 100%|██████████| 4000/4000 [15:07<00:00,  4.41it/s, 511 steps of size 5.51e-03. acc. prob=0.94] \n",
      "sample: 100%|██████████| 4000/4000 [14:18<00:00,  4.66it/s, 1023 steps of size 5.98e-03. acc. prob=0.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completing path 0 took 44.0 minutes\n",
      "running path 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2468.76it/s, init loss: 2765691.5000, avg. loss [9501-10000]: 64292.4922]\n",
      "sample: 100%|██████████| 4000/4000 [15:11<00:00,  4.39it/s, 511 steps of size 5.69e-03. acc. prob=0.94] \n",
      "sample: 100%|██████████| 4000/4000 [13:52<00:00,  4.80it/s, 1023 steps of size 6.25e-03. acc. prob=0.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completing path 1 took 43.0 minutes\n",
      "running path 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2510.42it/s, init loss: 2764940.0000, avg. loss [9501-10000]: 64171.7383]\n",
      "sample: 100%|██████████| 4000/4000 [14:26<00:00,  4.61it/s, 511 steps of size 5.94e-03. acc. prob=0.94] \n",
      "sample: 100%|██████████| 4000/4000 [14:10<00:00,  4.70it/s, 1023 steps of size 5.93e-03. acc. prob=0.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completing path 2 took 43.0 minutes\n",
      "running path 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2484.78it/s, init loss: 2762753.5000, avg. loss [9501-10000]: 64320.0742]\n",
      "sample: 100%|██████████| 4000/4000 [14:07<00:00,  4.72it/s, 511 steps of size 5.99e-03. acc. prob=0.93] \n",
      "sample: 100%|██████████| 4000/4000 [14:27<00:00,  4.61it/s, 895 steps of size 5.71e-03. acc. prob=0.93] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completing path 3 took 43.0 minutes\n",
      "running path 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2509.30it/s, init loss: 2764126.0000, avg. loss [9501-10000]: 64334.1172]\n",
      "sample: 100%|██████████| 4000/4000 [15:30<00:00,  4.30it/s, 383 steps of size 5.37e-03. acc. prob=0.94] \n",
      "sample: 100%|██████████| 4000/4000 [14:14<00:00,  4.68it/s, 1023 steps of size 5.76e-03. acc. prob=0.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completing path 4 took 44.0 minutes\n",
      "MAE: 2309.5895611738692, +/- 15.527420777382872\n",
      "R-squared: 0.18134906306973736, +/- 0.014495629715520248\n",
      "CPC: 0.5119379292225815, +/- 0.001165777378622975\n",
      "CPCD: 0.7500932521232849, +/- 0.005184211700233267\n"
     ]
    }
   ],
   "source": [
    "cpc_gr , cpcd_gr, mae_gr , r_squared_gr = np.empty(path_count), np.empty(path_count), np.empty(path_count), np.empty(path_count)\n",
    "posteriors_gr = []\n",
    "for path in df.path_ind.unique()[:path_count]:\n",
    "\n",
    "    X_train = df_train.query(\"path_ind == @path\")\n",
    "    y_train = np.log(1 + df_train.query(\"path_ind == @path\").M_ij.values)\n",
    "\n",
    "    tmp = np.log(1 + X_train.loc[:,'P_i':])\n",
    "    scaled_factors = pd.concat([X_train[['State_i','State_j']], \n",
    "                                1 / (1 + np.exp(- (tmp - tmp.mean(axis=0))\\\n",
    "                                    / tmp.std(ddof=1, axis=0) ))],\n",
    "                                axis=1)\n",
    "\n",
    "    features = {'population': ['P_i','P_j'],\n",
    "            'housing': ['H_i','H_j'],\n",
    "            'income': ['I_i','I_j'],\n",
    "            'affordability': ['AF_i','AF_j'],\n",
    "            'area': ['A_i','A_j'],\n",
    "            'density': ['rho_i','rho_j'],\n",
    "            'climate risk': ['ADC_i','ADC_j']\n",
    "    }\n",
    "\n",
    "    features_origin = list(np.array(list(features.values()))[:,0])\n",
    "    features_destination = list(np.array(list(features.values()))[:,1])\n",
    "\n",
    "    local_region_sum = np.empty((len(X_train), len(features)))\n",
    "    for i in range(len(X_train)):\n",
    "        intervening_states = states[distance[X_train.State_i.iloc[i]] < distance[X_train.State_i.iloc[i]][X_train.State_j.iloc[i]]]\\\n",
    "                            .drop(X_train.State_i.iloc[i])\n",
    "        if len(intervening_states):\n",
    "            local_region_sum[i] = np.array([scaled_factors.query(\"Year == {} & State_i == '{}'\"\\\n",
    "                                .format(scaled_factors.index[i], local_state))[features_origin].values[0]\n",
    "                                    for local_state in intervening_states]\n",
    "                            ).sum(axis=0)\n",
    "        else:\n",
    "            local_region_sum[i] = np.array([0.]*len(features))\n",
    "\n",
    "    guide = AutoDiagonalNormal(grm)\n",
    "    optimizer = numpyro.optim.Adam(step_size=1e-2)\n",
    "    svi = SVI(grm, guide, optimizer, Trace_ELBO())\n",
    "    svi_result = svi.run(random.PRNGKey(0), 10000, X_train, y_train, scaled_factors, local_region_sum, features_origin, features_destination)\n",
    "    neutra = NeuTraReparam(guide, svi_result.params)\n",
    "\n",
    "    reparam_grm = neutra.reparam(grm)\n",
    "    nuts_kernel = NUTS(reparam_grm)\n",
    "    \n",
    "    samples_num = 6000\n",
    "    chains_num = 2\n",
    "    mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=samples_num//chains_num, num_chains=chains_num, progress_bar=False)\n",
    "    mcmc.run(random.PRNGKey(0), X_train, y_train, scaled_factors, local_region_sum, features_origin, features_destination)\n",
    "\n",
    "    posteriors_gr += [neutra.transform_sample(mcmc.get_samples(group_by_chain=True)['auto_shared_latent'])]\n",
    "\n",
    "    X_test = df_test.query(\"path_ind == @path\")\n",
    "    y_test = df_test.query(\"path_ind == @path\").M_ij.values\n",
    "\n",
    "    tmp = np.log(1 + X_test.loc[:,'P_i':])\n",
    "    scaled_factors = pd.concat([X_test[['State_i','State_j']], \n",
    "                                1 / (1 + np.exp(- (tmp - tmp.mean(axis=0))\\\n",
    "                                    / tmp.std(ddof=1, axis=0) ))],\n",
    "                                axis=1)\n",
    "\n",
    "    local_region_sum = np.empty((len(X_test), len(features)))\n",
    "    for i in range(len(X_test)):\n",
    "        intervening_states = states[distance[X_test.State_i.iloc[i]] < distance[X_test.State_i.iloc[i]][X_test.State_j.iloc[i]]]\\\n",
    "                            .drop(X_test.State_i.iloc[i])\n",
    "        if len(intervening_states):\n",
    "            local_region_sum[i] = np.array([scaled_factors.query(\"Year == {} & State_i == '{}'\"\\\n",
    "                                .format(scaled_factors.index[i], local_state))[features_origin].values[0]\n",
    "                                    for local_state in intervening_states]\n",
    "                            ).sum(axis=0)\n",
    "        else:\n",
    "            local_region_sum[i] = np.array([0.]*len(features))\n",
    "\n",
    "    predictive = Predictive(reparam_grm, posteriors_gr[-1], return_sites=[\"flow\"])\n",
    "    samples_predictive = predictive(random.PRNGKey(0), X_test, None, scaled_factors, local_region_sum, features_origin, features_destination)\n",
    "\n",
    "    pred = np.exp( samples_predictive[\"flow\"].mean(axis=0) ) - 1\n",
    "\n",
    "    cpc_gr[path] = cpc(y_test, pred) \n",
    "    cpcd_gr[path] = cpcd(y_test, pred, df_test.query(\"path_ind == @path\").D_ij) \n",
    "    mae_gr[path] = mae(y_test, pred )\n",
    "    r_squared_gr[path] = r_squared(y_test, pred)\n",
    "\n",
    "print(\"MAE: {}, +/- {}\".format(mae_gr.mean(), norm.ppf(.975) * mae_gr.std(ddof=1) / np.sqrt(path_count) ) )\n",
    "print(\"R-squared: {}, +/- {}\".format(r_squared_gr.mean(), norm.ppf(.975) * r_squared_gr.std(ddof=1) / np.sqrt(path_count) ) )\n",
    "print(\"CPC: {}, +/- {}\".format(cpc_gr.mean(), norm.ppf(.975) * cpc_gr.std(ddof=1) / np.sqrt(path_count) ) )\n",
    "print(\"CPCD: {}, +/- {}\".format(cpcd_gr.mean(), norm.ppf(.975) * cpcd_gr.std(ddof=1) / np.sqrt(path_count) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/posteriors_gr.pkl', 'wb') as f:\n",
    "    pickle.dump(posteriors_gr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
